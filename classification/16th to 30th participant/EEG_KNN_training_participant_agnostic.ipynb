{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pfhc1MRC4X4m","collapsed":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib as plt\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the components of your path\n","drive_base = '/content/drive'\n","my_drive = 'MyDrive'\n","project_folder = 'EEG PE'\n","target_folder = 'Users_data'\n","# subject = 16\n","\n","# Initialize lists to store data\n","all_X = []\n","all_y = []\n","\n","for subject in range(16, 31):\n","  # Construct the full path using os.path.join\n","  full_path = os.path.join(drive_base, my_drive, project_folder, target_folder,str(subject))\n","  # print(\"Constructed path:\", full_path)\n","\n","  # Check if the folder exists\n","  if os.path.exists(full_path):\n","      # print(f\"Folder exists at: {full_path}\")\n","      os.chdir(full_path)\n","      # print(\"Changed directory to:\", os.getcwd())\n","  # else:\n","  #     print(f\"Folder does not exist at: {full_path}\")\n","\n","  # List of filenames for each dataset\n","  p_files = [\"features_1_1.csv\", \"features_1_4.csv\", \"features_1_5.csv\", \"features_1_6.csv\", \"features_1_7.csv\", \"features_1_8.csv\"]\n","  c_files = [\"features_2_1.csv\", \"features_2_4.csv\", \"features_2_5.csv\", \"features_2_6.csv\", \"features_2_7.csv\", \"features_2_8.csv\"]\n","\n","  # Function to read CSV files into a list of DataFrames\n","  def read_csv_files(file_list):\n","      dataframes = []\n","      for file in file_list:\n","          df = pd.read_csv(file)\n","          # Extract just the time part of 'ftime' and store it in a new column\n","          df['ftime_only'] = pd.to_datetime(df['ftime']).dt.strftime('%H:%M:%S')\n","          dataframes.append(df)\n","      return dataframes\n","\n","\n","  # Read all files into lists of DataFrames\n","  p_data = read_csv_files(p_files)\n","  c_data = read_csv_files(c_files)\n","\n","  # Function to read CSV files into a list of DataFrames\n","  def read_csv_files(file_list):\n","      return [pd.read_csv(file) for file in file_list]\n","\n","  # Read all files into lists of DataFrames\n","  p_data = read_csv_files(p_files)\n","  c_data = read_csv_files(c_files)\n","\n","  # Function to merge DataFrames in pairs\n","  def merge_dataframes(data_list):\n","      merged_data = data_list[0]\n","      for i in range(1, len(data_list)):\n","          merged_data = pd.merge(merged_data, data_list[i], how=\"inner\", on=[\"ftime\"], suffixes=('', f'_file{i}'))\n","\n","      return merged_data\n","\n","  # Merge DataFrames for each dataset\n","  p_merged = merge_dataframes(p_data)\n","  c_merged = merge_dataframes(c_data)\n","\n","  # Function to clean the merged DataFrame\n","  def clean_dataframe(df):\n","      # Ensure 'ftime' is retained and formatted\n","      if 'ftime' in df.columns:\n","          df['ftime'] = pd.to_datetime(df['ftime']).dt.strftime('%H:%M:%S')\n","\n","      df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Drop 'Unnamed' columns\n","      df = df.replace(np.nan, 0)                            # Replace NaNs with 0\n","      df = df.replace(np.inf, 10000)                        # Replace +Inf with a large value\n","      df = df.replace(-np.inf, -999999)                     # Replace -Inf with a large negative value\n","\n","      return df\n","\n","  # Clean the merged DataFrames\n","  pmerge5 = clean_dataframe(p_merged)\n","  cmerge5 = clean_dataframe(c_merged)\n","\n","  cmerge5['ftime'] = pd.to_datetime(cmerge5['ftime']).dt.strftime('%H:%M:%S')\n","  pmerge5['ftime'] = pd.to_datetime(pmerge5['ftime']).dt.strftime('%H:%M:%S')\n","\n","  # print(cmerge5.shape)\n","  # print(pmerge5.shape)\n","\n","  timestampsfoldername = 'Timestamps'\n","  stampsfolder = 'stamps'\n","  syncedtimecsvfile = 'Synced_Times_All.csv'\n","\n","  full_path = os.path.join(drive_base, my_drive, project_folder, timestampsfoldername,stampsfolder)\n","  # print(\"Constructed path:\", full_path)\n","\n","  # Check if the folder exists\n","  if os.path.exists(full_path):\n","      # print(f\"Folder exists at: {full_path}\")\n","      os.chdir(full_path)\n","      # print(\"Changed directory to:\", os.getcwd())\n","  # else:\n","  #     print(f\"Folder does not exist at: {full_path}\")\n","\n","  df = pd.read_csv(syncedtimecsvfile)\n","\n","  offsets = {}\n","  for index, row in df.iterrows():\n","      # Assuming the participant number is extracted from the 'File Name' and matches the participant id directly\n","      participant_id = int(row['File Name'][:2]) # Modify this line if the format is different\n","\n","      # Extracting and converting the offset time from '(hh, mm, ss)' to 'hh:mm:ss'\n","      offset_time = row['Offset Time']\n","      offset_time_clean = offset_time.strip('()')  # Remove parentheses\n","      hh, mm, ss = map(int, offset_time_clean.split(', '))  # Split and convert to integers\n","      formatted_time = f\"{hh:02d}:{mm:02d}:{ss:02d}\"  # Format to 'hh:mm:ss'\n","\n","      offsets[participant_id] = formatted_time\n","\n","  #Construct the full path using os.path.join\n","  full_path = os.path.join(drive_base, my_drive, project_folder, target_folder,str(subject))\n","  # print(\"Constructed path:\", full_path)\n","\n","  # Check if the folder exists\n","  if os.path.exists(full_path):\n","      # print(f\"Folder exists at: {full_path}\")\n","      os.chdir(full_path)\n","      # print(\"Changed directory to:\", os.getcwd())\n","  # else:\n","  #     print(f\"Folder does not exist at: {full_path}\")\n","  offsets\n","\n","  import pandas as pd\n","  from datetime import timedelta\n","\n","  full_path = os.path.join(drive_base, my_drive, project_folder, timestampsfoldername,stampsfolder)\n","  # print(\"Constructed path:\", full_path)\n","\n","  # Check if the folder exists\n","  if os.path.exists(full_path):\n","      # print(f\"Folder exists at: {full_path}\")\n","      os.chdir(full_path)\n","      # print(\"Changed directory to:\", os.getcwd())\n","  # else:\n","  #     print(f\"Folder does not exist at: {full_path}\")\n","\n","  # Load CSV file\n","  df = pd.read_csv('trainingStamps.csv')\n","\n","  # Strip leading/trailing spaces from column names\n","  df.columns = df.columns.str.strip()\n","\n","  # Replace invalid values like '-' with NaN\n","  # df['Start TIme (minutes)'] = df['Start'].replace('-', pd.NA)\n","  # df['End Time (minutes)'] = df['End'].replace('-', pd.NA)\n","\n","  df['Start TIme (minutes)'] = df['Start']\n","  df['End Time (minutes)'] = df['End']\n","\n","  df = df.drop('Start', axis=1)\n","  df = df.drop('End', axis=1)\n","\n","  # Convert valid time strings to datetime, using errors='coerce' to handle invalid formats\n","  # df['Start TIme (minutes)'] = pd.to_datetime(df['Start TIme (minutes)'], format='%M:%S', errors='coerce').dt.time\n","  # df['End Time (minutes)'] = pd.to_datetime(df['End Time (minutes)'], format='%M:%S', errors='coerce').dt.time\n","\n","  # Display the first 5 rows and the column names\n","  # print(df.head(5))\n","  # print(df.columns)\n","\n","  #Construct the full path using os.path.join\n","  full_path = os.path.join(drive_base, my_drive, project_folder, target_folder,str(subject))\n","  # print(\"Constructed path:\", full_path)\n","\n","  # Check if the folder exists\n","  if os.path.exists(full_path):\n","      # print(f\"Folder exists at: {full_path}\")\n","      os.chdir(full_path)\n","      # print(\"Changed directory to:\", os.getcwd())\n","  # else:\n","      # print(f\"Folder does not exist at: {full_path}\")\n","\n","  from datetime import timedelta, datetime\n","\n","  # Filter rows for the specific subject\n","  training_time_df = df[df['Subject'] == subject]\n","\n","\n","  # Function to convert offset string to timedelta\n","  def offset_to_timedelta(offset_str):\n","      h, m, s = map(int, offset_str.split(':'))\n","      return timedelta(hours=h, minutes=m, seconds=s)\n","\n","  def string_to_time(time_str):\n","      return datetime.strptime(time_str, '%H:%M:%S').time()\n","\n","  # Apply the offset for the selected subject\n","  if subject in offsets:\n","      time_offset = offset_to_timedelta(offsets[subject])\n","\n","      # Add the offset to the 'Start Time (minutes)' and 'End Time (minutes)' columns\n","      training_time_df['Start TIme (minutes)'] = training_time_df['Start TIme (minutes)'].apply(\n","          lambda t: (datetime.combine(datetime.today(), string_to_time(t)) + time_offset).time() if pd.notna(t) else t\n","      )\n","      training_time_df['End Time (minutes)'] = training_time_df['End Time (minutes)'].apply(\n","          lambda t: (datetime.combine(datetime.today(), string_to_time(t)) + time_offset).time() if pd.notna(t) else t\n","      )\n","\n","  # print(training_time_df.shape)\n","\n","  cmerge5['ftime'] = pd.to_datetime(cmerge5['ftime'], format='%H:%M:%S')\n","  training_time_df['Start TIme (minutes)'] = pd.to_datetime(training_time_df['Start TIme (minutes)'], format='%H:%M:%S')\n","  training_time_df['End Time (minutes)'] = pd.to_datetime(training_time_df['End Time (minutes)'], format='%H:%M:%S')\n","\n","  filtered_cmerge5 = pd.DataFrame()\n","\n","  # Loop over each row in training_time_df\n","  for _, row in training_time_df.iterrows():\n","      if row['File'] == '3.py' and row['Task'] == 2 :\n","        start_time = row['Start TIme (minutes)']\n","        end_time = row['End Time (minutes)']\n","\n","        # Filter cmerge5 for rows where 'ftime' is within the time interval\n","        filtered_rows = cmerge5[(cmerge5['ftime'] >= start_time) & (cmerge5['ftime'] <= end_time)]\n","\n","        # Append the filtered rows to the result dataframe\n","        filtered_cmerge5 = pd.concat([filtered_cmerge5, filtered_rows])\n","\n","  # Reset index after filtering\n","  filtered_cmerge5 = filtered_cmerge5.reset_index(drop=True)\n","  filtered_cmerge5 = filtered_cmerge5.drop(columns='ftime')\n","  filtered_cmerge5['label'] = 1\n","  # print(filtered_cmerge5.shape)\n","  # print(filtered_cmerge5.head(5))\n","\n","  pmerge5['ftime'] = pd.to_datetime(pmerge5['ftime'], format='%H:%M:%S')\n","  training_time_df['Start TIme (minutes)'] = pd.to_datetime(training_time_df['Start TIme (minutes)'], format='%H:%M:%S')\n","  training_time_df['End Time (minutes)'] = pd.to_datetime(training_time_df['End Time (minutes)'], format='%H:%M:%S')\n","\n","  filtered_cmerge5_flag0 = pd.DataFrame()\n","\n","  # Loop over each row in training_time_df\n","  for _, row in training_time_df.iterrows():\n","      if row['Task'] == 1:\n","        start_time = row['Start TIme (minutes)']\n","        end_time = row['End Time (minutes)']\n","\n","\n","        # Filter pmerge5 for rows where 'ftime' is within the time interval\n","        filtered_rows = pmerge5[(pmerge5['ftime'] >= start_time) & (pmerge5['ftime'] <= end_time)]\n","\n","        # Append the filtered rows to the result dataframe\n","        filtered_cmerge5_flag0 = pd.concat([filtered_cmerge5_flag0, filtered_rows])\n","\n","  # Reset index after filtering\n","  filtered_cmerge5_flag0 = filtered_cmerge5_flag0.reset_index(drop=True)\n","  # print(filtered_cmerge5_flag0)\n","  filtered_cmerge5_flag0 = filtered_cmerge5_flag0.drop(columns='ftime')\n","  filtered_cmerge5_flag0['label'] =  0\n","  # print(filtered_cmerge5_flag0.shape)\n","  # print(filtered_cmerge5_flag0.head(5))\n","\n","\n","  combined_df = pd.concat([filtered_cmerge5, filtered_cmerge5_flag0], ignore_index=True)\n","  shuffled_df = combined_df.sample(frac=1).reset_index(drop=True)\n","  dataset = shuffled_df.copy()\n","  # print(dataset.shape)\n","\n","\n","  X = dataset.iloc[:, :-1].values\n","  y = dataset.iloc[:,-1].values\n","\n","  # Append the results to the list\n","  all_X.append(X)\n","  all_y.append(y)\n","  print(\"Done subject: \", subject)"]},{"cell_type":"markdown","metadata":{"id":"YvxIPVyMhmKp"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"AVzJWAXIhxoC","executionInfo":{"status":"ok","timestamp":1729048199765,"user_tz":-330,"elapsed":4,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","# Assuming all_X is a list of numpy arrays\n","final_X = np.vstack(all_X)  # This stacks all the arrays vertically (by rows)\n","\n","# If all_y is a list of 1D numpy arrays or lists\n","final_y = np.concatenate(all_y)\n","X_train, X_test, y_train, y_test = train_test_split(final_X, final_y, test_size = 0.25, random_state = 1)"]},{"cell_type":"markdown","metadata":{"id":"3qSuQIp9xWTv"},"source":["# Feature Scaling"]},{"cell_type":"code","source":["# Example to check the shape of each item in X_train\n","for i, x in enumerate(X_train):\n","    print(f\"Shape of row {i}: {np.array(x).shape}\")\n"],"metadata":{"id":"XRZ-rUXmOTtz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"K6AMaoRWxZJg","executionInfo":{"status":"ok","timestamp":1729048200415,"user_tz":-330,"elapsed":2,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"bb6jCOCQiAmP"},"source":["## Training the K-NN model on the Training set"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0pFVAmciHQs","outputId":"54f78494-92f9-40fc-c7e6-13a6405c8b36","executionInfo":{"status":"ok","timestamp":1729048277496,"user_tz":-330,"elapsed":936,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[71  0]\n"," [ 2 19]]\n","Accuracy: 82.76% k=1\n","[[71  0]\n"," [ 3 18]]\n","Accuracy: 100.00% k=2\n","[[71  0]\n"," [ 3 18]]\n","Accuracy: 100.00% k=3\n","[[71  0]\n"," [ 5 16]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 100.00% k=4\n","[[70  1]\n"," [ 4 17]]\n","Accuracy: 100.00% k=5\n","[[70  1]\n"," [ 6 15]]\n","Accuracy: 100.00% k=6\n","[[69  2]\n"," [ 6 15]]\n","Accuracy: 100.00% k=7\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[[69  2]\n"," [ 9 12]]\n","Accuracy: 100.00% k=8\n","[[69  2]\n"," [ 8 13]]\n","Accuracy: 100.00% k=9\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n","  warnings.warn(\n"]}],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","for k in range(1, 10):\n","\n","  classifier = KNeighborsClassifier(n_neighbors = k , metric = 'minkowski', p = 2)\n","  classifier.fit(X_train, y_train)\n","\n","  y_pred = classifier.predict(X_test)\n","  # print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n","\n","  from sklearn.metrics import confusion_matrix, accuracy_score\n","  cm = confusion_matrix(y_test, y_pred)\n","  print(cm)\n","  accuracy_score(y_test, y_pred)\n","\n","  import pandas as pd\n","\n","  test1 = pd.DataFrame()\n","\n","  # Loop over each row in training_time_df\n","  for _, row in training_time_df.iterrows():\n","\n","      if (row['File'] != '1.py' and row['Task'] == 1):\n","        start_time = row['Start TIme (minutes)']\n","        end_time = row['End Time (minutes)']\n","\n","        # Filter cmerge5 for rows where 'ftime' is within the time interval\n","        filtered_rows = pmerge5[(pmerge5['ftime'] >= start_time) & (pmerge5['ftime'] <= end_time)]\n","\n","        # Append the filtered rows to the result dataframe\n","        test1 = pd.concat([test1, filtered_rows])\n","\n","      if (row['File'] == '1.py' and row['Task'] == 2):\n","          start_time = row['Start TIme (minutes)']\n","          end_time = row['End Time (minutes)']\n","\n","          # Filter cmerge5 for rows where 'ftime' is within the time interval\n","          filtered_rows = cmerge5[(cmerge5['ftime'] >= start_time) & (cmerge5['ftime'] <= end_time)]\n","\n","          # Append the filtered rows to the result dataframe\n","          test1 = pd.concat([test1, filtered_rows])\n","\n","\n","  # Reset index after filtering\n","  test1 = test1.reset_index(drop=True)\n","\n","  # Optionally, drop 'ftime' column and add 'label' column if needed\n","  test1 = test1.drop(columns='ftime')\n","  # print(test1.shape)\n","\n","  test1 = sc.transform(test1)\n","  y_pred_unknown = classifier.predict(test1)\n","  # print(y_pred_unknown)\n","\n","  accuracy = accuracy_score(y_pred_unknown, np.zeros(len(y_pred_unknown)))\n","  print(f\"Accuracy: {accuracy * 100:.2f}% k={k}\")"]},{"cell_type":"markdown","metadata":{"id":"vKYVQH-l5NpE"},"source":["## Predicting the Test set results"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"p6VMTb2O4hwM","executionInfo":{"status":"aborted","timestamp":1729048168871,"user_tz":-330,"elapsed":6,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"h4Hwj34ziWQW"},"source":["## Making the Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6bpZwUiiXic","executionInfo":{"status":"aborted","timestamp":1729048168871,"user_tz":-330,"elapsed":6,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"M6PXkLDiPIhS"},"source":["# Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78JF51qtPOME","executionInfo":{"status":"aborted","timestamp":1729048168871,"user_tz":-330,"elapsed":6,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SloeGYyP68I","executionInfo":{"status":"aborted","timestamp":1729048168871,"user_tz":-330,"elapsed":6,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBXOxqGyWlUh","executionInfo":{"status":"aborted","timestamp":1729048168872,"user_tz":-330,"elapsed":7,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"outputs":[],"source":[]},{"cell_type":"code","source":["for k in range(1, 10):  # Example: Trying different k values from 1 to 9\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    classifier.fit(X_train, y_train)\n","    print(f\"Accuracy for k={k}: {knn.score(X_train, y_test)}\")"],"metadata":{"id":"qIvLoIwjSA8e","executionInfo":{"status":"aborted","timestamp":1729048168872,"user_tz":-330,"elapsed":7,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}